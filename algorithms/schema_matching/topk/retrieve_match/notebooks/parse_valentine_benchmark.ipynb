{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf88e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e93d2a",
   "metadata": {},
   "source": [
    "## Fabricated Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3723234b-10b7-4406-9084-3842f7d6ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(path, new_dir):\n",
    "    json_files = [pos_json for pos_json in os.listdir(path) if \"mapping\" in pos_json]\n",
    "    if not json_files:\n",
    "        return\n",
    "    json_file = json_files[0]\n",
    "    \n",
    "    tables = [table for table in os.listdir(path) if table.endswith(\".csv\")]\n",
    "    \n",
    "    source_tab = [table for table in tables if \"source\" in table][0]\n",
    "    target_tab = [table for table in tables if \"target\" in table][0]\n",
    "    \n",
    "    df_dict = {\n",
    "        \"source_col\": [],\n",
    "        \"target_col\": [],\n",
    "        \"source_tab\": [],\n",
    "        \"target_tab\": []\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(path, json_file)) as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "        for match in d['matches']:\n",
    "            print(match[\"source_column\"], match[\"target_column\"])\n",
    "            df_dict[\"source_col\"].append(match[\"source_column\"])\n",
    "            df_dict[\"target_col\"].append(match[\"target_column\"])\n",
    "            df_dict[\"source_tab\"].append(source_tab.split(\".\")[0])\n",
    "            df_dict[\"target_tab\"].append(target_tab)\n",
    "            \n",
    "\n",
    "    source_df = pd.read_csv(os.path.join(path, source_tab))\n",
    "    target_df = pd.read_csv(os.path.join(path, target_tab))\n",
    "    source_df.to_csv(os.path.join(new_dir, \"source-tables\", source_tab), index=False)\n",
    "    target_df.to_csv(os.path.join(new_dir, \"target-tables\", target_tab), index=False)\n",
    "\n",
    "    return pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc4304ad-5f16-4b9c-b5e0-331d68897731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChEMBL\n",
      "Joinable\n",
      "Semantically-Joinable\n",
      "Unionable\n",
      "View-Unionable\n",
      "OpenData\n",
      "Joinable\n",
      "Semantically-Joinable\n",
      "Unionable\n",
      "View-Unionable\n",
      "TPC-DI\n",
      "Joinable\n",
      "Semantically-Joinable\n",
      "Unionable\n",
      "View-Unionable\n",
      "Wikidata\n",
      "Joinable\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Valentine-datasets/Wikidata/Joinable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         ground_truth \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([ground_truth, df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikidata\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mread_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     ground_truth \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([ground_truth, df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m ground_truth\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(new_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mread_json_file\u001b[0;34m(path, new_dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_json_file\u001b[39m(path, new_dir):\n\u001b[0;32m----> 2\u001b[0m     json_files \u001b[38;5;241m=\u001b[39m [pos_json \u001b[38;5;28;01mfor\u001b[39;00m pos_json \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pos_json]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_files:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Valentine-datasets/Wikidata/Joinable'"
     ]
    }
   ],
   "source": [
    "types = [\"Joinable\", \"Semantically-Joinable\", \"Unionable\", \"View-Unionable\"]\n",
    "datasets = [\"ChEMBL\", \"OpenData\", \"TPC-DI\", \"Wikidata\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    for type in types:\n",
    "        print(type)\n",
    "        root = \"./Valentine-datasets/\" + dataset + \"/\" + type\n",
    "        new_dir = \"./valentine/\" + dataset + \"/\" + type\n",
    "\n",
    "        if not os.path.exists(new_dir):\n",
    "            os.makedirs(new_dir)\n",
    "            os.makedirs(new_dir + \"/source-tables\")\n",
    "            os.makedirs(new_dir + \"/target-tables\")\n",
    "\n",
    "        ground_truth = pd.DataFrame(columns=[\"source_col\", \"target_col\", \"source_tab\", \"target_tab\"])\n",
    "\n",
    "        for _, dirs, _ in os.walk(root):\n",
    "            for dir in dirs:\n",
    "                print(dir)\n",
    "                df = read_json_file(os.path.join(root, dir), new_dir)\n",
    "                if df is None:\n",
    "                    continue\n",
    "\n",
    "                ground_truth = pd.concat([ground_truth, df], ignore_index=True)\n",
    "                \n",
    "        # if dataset == \"Wikidata\":\n",
    "        #     df = read_json_file(root, new_dir)\n",
    "        #     ground_truth = pd.concat([ground_truth, df], ignore_index=True)\n",
    "\n",
    "        ground_truth.to_csv(os.path.join(new_dir, \"matches.csv\"), index=False)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997e5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_google_exp\n",
      "id id\n",
      "title title\n",
      "manufacturer manufacturer\n",
      "price price\n",
      "dblp_acm\n",
      "id id\n",
      "title title\n",
      "authors authors\n",
      "venue venue\n",
      "year year\n",
      "walmart_amazon\n",
      "id id\n",
      "title title\n",
      "category category\n",
      "brand brand\n",
      "modelno modelno\n",
      "price price\n",
      "dblp_scholar\n",
      "id id\n",
      "title title\n",
      "authors authors\n",
      "venue venue\n",
      "year year\n",
      "beeradvo_ratebeer\n",
      "id id\n",
      "Beer_Name Beer_Name\n",
      "Brew_Factory_Name Brew_Factory_Name\n",
      "Style Style\n",
      "ABV ABV\n",
      "itunes_amazon\n",
      "id id\n",
      "Song_Name Song_Name\n",
      "Artist_Name Artist_Name\n",
      "Album_Name Album_Name\n",
      "Genre Genre\n",
      "Price Price\n",
      "CopyRight CopyRight\n",
      "Time Time\n",
      "Released Released\n",
      "fodors_zagats\n",
      "id id\n",
      "name name\n",
      "addr addr\n",
      "city city\n",
      "phone phone\n",
      "type type\n",
      "class class\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Magellan\"\n",
    "root = \"./Valentine-datasets/\" + dataset\n",
    "new_dir = \"./valentine/\" + dataset\n",
    "\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "    os.makedirs(new_dir + \"/source-tables\")\n",
    "    os.makedirs(new_dir + \"/target-tables\")\n",
    "\n",
    "ground_truth = pd.DataFrame(columns=[\"source_col\", \"target_col\", \"source_tab\", \"target_tab\"])\n",
    "\n",
    "for _, dirs, _ in os.walk(root):\n",
    "    for dir in dirs:\n",
    "        print(dir)\n",
    "        df = read_json_file(os.path.join(root, dir), new_dir)\n",
    "        if df is None:\n",
    "            continue\n",
    "\n",
    "        ground_truth = pd.concat([ground_truth, df], ignore_index=True)\n",
    "\n",
    "ground_truth.to_csv(os.path.join(new_dir, \"matches.csv\"), index=False)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57552c1d-0387-4173-8da3-cb88363f36c9",
   "metadata": {},
   "source": [
    "## Biomedical Pretrain: Create joined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98575394-3e1a-470c-b214-75eccbf66a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "usecases = [\"cao\", \"dou\", \"clark\", \"huang\", \"krug\", \"satpathy\", \"vasaikar\", \"wang\"]\n",
    "\n",
    "df_all = pd.DataFrame({})\n",
    "for usecase in usecases:\n",
    "    df = pd.read_csv(os.path.join(usecase, \"source.csv\")).sample(50)\n",
    "    df_all = pd.concat([df_all, df], sort=False)\n",
    "df_all\n",
    "\n",
    "df_all.to_csv(\"gdc_all/source.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d093907c-cbd9-4439-8eb9-68f2385ede2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_usecases = [\"cao\", \"dou\", \"clark\", \"huang\"]\n",
    "\n",
    "gt_all = pd.DataFrame({})\n",
    "\n",
    "for usecase in train_usecases:\n",
    "    df = pd.read_csv(os.path.join(usecase, \"groundtruth.csv\"))\n",
    "    gt_all = pd.concat([gt_all, df], sort=False)\n",
    "gt_all = gt_all.drop_duplicates()\n",
    "gt_all\n",
    "\n",
    "gt_all.to_csv(\"gdc_all/groundtruth.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e6dd0-5256-4ce9-9e1c-47ecd2e0ee52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stransformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
